{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classification and transfer learning with keras-bert - MULTICLASS",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqMpI159_056"
      },
      "source": [
        "# About\n",
        "\n",
        "keras-bert"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj-zH981_058"
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNgNOCLaAOgo",
        "outputId": "9240e32c-2e07-489f-e1f4-43397933a4da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3GVKNPD_05-"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oD1svI-0VINQ",
        "outputId": "9eb613d7-781d-4b5d-9db8-04c6f0a82bc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        }
      },
      "source": [
        "pip install keras-bert\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-bert\n",
            "  Downloading https://files.pythonhosted.org/packages/e2/7f/95fabd29f4502924fa3f09ff6538c5a7d290dfef2c2fe076d3d1a16e08f0/keras-bert-0.86.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-bert) (1.18.5)\n",
            "Requirement already satisfied: Keras>=2.4.3 in /usr/local/lib/python3.6/dist-packages (from keras-bert) (2.4.3)\n",
            "Collecting keras-transformer>=0.38.0\n",
            "  Downloading https://files.pythonhosted.org/packages/89/6c/d6f0c164f4cc16fbc0d0fea85f5526e87a7d2df7b077809e422a7e626150/keras-transformer-0.38.0.tar.gz\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.4.3->keras-bert) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.4.3->keras-bert) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.4.3->keras-bert) (2.10.0)\n",
            "Collecting keras-pos-embd>=0.11.0\n",
            "  Downloading https://files.pythonhosted.org/packages/09/70/b63ed8fc660da2bb6ae29b9895401c628da5740c048c190b5d7107cadd02/keras-pos-embd-0.11.0.tar.gz\n",
            "Collecting keras-multi-head>=0.27.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e6/32/45adf2549450aca7867deccfa04af80a0ab1ca139af44b16bc669e0e09cd/keras-multi-head-0.27.0.tar.gz\n",
            "Collecting keras-layer-normalization>=0.14.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/0e/d1078df0494bac9ce1a67954e5380b6e7569668f0f3b50a9531c62c1fc4a/keras-layer-normalization-0.14.0.tar.gz\n",
            "Collecting keras-position-wise-feed-forward>=0.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e3/59/f0faa1037c033059e7e9e7758e6c23b4d1c0772cd48de14c4b6fd4033ad5/keras-position-wise-feed-forward-0.6.0.tar.gz\n",
            "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /simple/keras-embed-sim/\u001b[0m\n",
            "Collecting keras-embed-sim>=0.8.0\n",
            "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /packages/57/ef/61a1e39082c9e1834a2d09261d4a0b69f7c818b359216d4e1912b20b1c86/keras-embed-sim-0.8.0.tar.gz\u001b[0m\n",
            "  Downloading https://files.pythonhosted.org/packages/57/ef/61a1e39082c9e1834a2d09261d4a0b69f7c818b359216d4e1912b20b1c86/keras-embed-sim-0.8.0.tar.gz\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->Keras>=2.4.3->keras-bert) (1.15.0)\n",
            "Collecting keras-self-attention==0.46.0\n",
            "  Downloading https://files.pythonhosted.org/packages/15/6b/c804924a056955fa1f3ff767945187103cfc851ba9bd0fc5a6c6bc18e2eb/keras-self-attention-0.46.0.tar.gz\n",
            "Building wheels for collected packages: keras-bert, keras-transformer, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-self-attention\n",
            "  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-bert: filename=keras_bert-0.86.0-cp36-none-any.whl size=34145 sha256=47a212e17a9947dada5c3e1bfca7fd857052f507b03caedfe32964097b971e35\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/f0/b1/748128b58562fc9e31b907bb5e2ab6a35eb37695e83911236b\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-transformer: filename=keras_transformer-0.38.0-cp36-none-any.whl size=12942 sha256=ac5aa4a8136b6d4498d4d3d2e539fb94789ee3323cb1f7217006e0149af48320\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/fb/3a/37b2b9326c799aa010ae46a04ddb04f320d8c77c0b7e837f4e\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.11.0-cp36-none-any.whl size=7554 sha256=ebaf75a8a870bc848652f1498db5493f9f2e324efe63f1a689499900e5893083\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/a1/a0/ce6b1d49ba1a9a76f592e70cf297b05c96bc9f418146761032\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-multi-head: filename=keras_multi_head-0.27.0-cp36-none-any.whl size=15612 sha256=815242b8ebecf413774fb17333f14577e85389b097e9f78722292d57c5b147a5\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/b4/49/0a0c27dcb93c13af02fea254ff51d1a43a924dd4e5b7a7164d\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.14.0-cp36-none-any.whl size=5268 sha256=7bef5ed5596d1f769b627988e24715cb813e0a92d078f0453a16b864488a9dbc\n",
            "  Stored in directory: /root/.cache/pip/wheels/54/80/22/a638a7d406fd155e507aa33d703e3fa2612b9eb7bb4f4fe667\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.6.0-cp36-none-any.whl size=5626 sha256=ddf40ffd46428d2459be2b32d961cb867f455d246e1516a3e0c6a70c0e7504a8\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/e2/e2/3514fef126a00574b13bc0b9e23891800158df3a3c19c96e3b\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.8.0-cp36-none-any.whl size=4559 sha256=8430b7944001b3426704c88f0023c69006f04e3eecffcbf1331b34a8bdcd6a21\n",
            "  Stored in directory: /root/.cache/pip/wheels/49/45/8b/c111f6cc8bec253e984677de73a6f4f5d2f1649f42aac191c8\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.46.0-cp36-none-any.whl size=17278 sha256=3db0210e81b18631e91c6cf8f6a578bc7043f02b86da0fe51047a7b860b9eab1\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/2e/80/fec4c05eb23c8e13b790e26d207d6e0ffe8013fad8c6bdd4d2\n",
            "Successfully built keras-bert keras-transformer keras-pos-embd keras-multi-head keras-layer-normalization keras-position-wise-feed-forward keras-embed-sim keras-self-attention\n",
            "Installing collected packages: keras-pos-embd, keras-self-attention, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-transformer, keras-bert\n",
            "Successfully installed keras-bert-0.86.0 keras-embed-sim-0.8.0 keras-layer-normalization-0.14.0 keras-multi-head-0.27.0 keras-pos-embd-0.11.0 keras-position-wise-feed-forward-0.6.0 keras-self-attention-0.46.0 keras-transformer-0.38.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75lTilUo_06i"
      },
      "source": [
        "## prepare dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiVbQtvn_06j"
      },
      "source": [
        "vocab_size = 283 # includes special tokens\n",
        "# Pretend maxlen is 20\n",
        "maxlen = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtCdtidEAlK6"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9qj03rWAW2h"
      },
      "source": [
        "train = pd.read_csv(\"/content/drive/My Drive/Research/CyberBERT/data/train_multiclass.csv\")\n",
        "valid = pd.read_csv(\"/content/drive/My Drive/Research/CyberBERT/data/valid_multiclass.csv\")\n",
        "test = pd.read_csv(\"/content/drive/My Drive/Research/CyberBERT/data/test_multiclass.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jfjim6PDvmr"
      },
      "source": [
        "def split_and_convert(row):\n",
        "\n",
        "  return [int(x) for x in row.split(\" \")]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_n9iPRiDR6i"
      },
      "source": [
        "train[\"calls\"] = train[\"calls\"].apply(lambda x: split_and_convert(x))\n",
        "valid[\"calls\"] = valid[\"calls\"].apply(lambda x: split_and_convert(x))\n",
        "test[\"calls\"] = test[\"calls\"].apply(lambda x: split_and_convert(x))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMGWuj3PAo3c"
      },
      "source": [
        "y_train = np.asarray(train[\"label\"])\n",
        "x_train = np.stack(np.asarray(train[\"calls\"]),axis=0)\n",
        "\n",
        "y_val = np.asarray(valid[\"label\"])\n",
        "x_val = np.stack(np.asarray(valid[\"calls\"]),axis=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHhqxAHuFloO"
      },
      "source": [
        "y_test = np.asarray(test[\"label\"])\n",
        "x_test = np.stack(np.asarray(test[\"calls\"]),axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L35T4E_nZgzC"
      },
      "source": [
        "## pre-train a BERT model on our data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igd_aA7XZtHU"
      },
      "source": [
        "from random import randrange\n",
        "\n",
        "sep_id = randrange(10,90)\n",
        "\n",
        "training_input = []\n",
        "\n",
        "for _, row in train[[\"calls\"]].iterrows():\n",
        "  sep_id = randrange(10,90)\n",
        "  row_value = row[0]\n",
        "  list_input = [row_value[:sep_id], row_value[sep_id:]]\n",
        "  training_input.append(list_input)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K18DPBzwVpWw"
      },
      "source": [
        "import keras\n",
        "from keras_bert import get_base_dict, get_model, compile_model, gen_batch_inputs\n",
        "\n",
        "\n",
        "# Use training_input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIejIg-gZm0z",
        "outputId": "e37c7b67-a18b-4a80-84f3-a0c284e17432",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "### SET MAX LEN\n",
        "maxlen = 100\n",
        "\n",
        "# Build token dictionary\n",
        "token_dict = get_base_dict()   # A dict that contains some special tokens\n",
        "for pairs in training_input:\n",
        "    for token in pairs[0] + pairs[1]:\n",
        "        if token not in token_dict:\n",
        "            token_dict[token] = len(token_dict)\n",
        "token_list = list(token_dict.keys())  # Used for selecting a random word\n",
        "len(token_dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "253"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyvwI7dRt_AC"
      },
      "source": [
        "# # Ensure token dict has all the required tokens\n",
        "## ONLY NEEDED IF TRIMMING\n",
        "# for token in x_train.flatten():\n",
        "#   if token not in token_dict:\n",
        "#     token_dict[token] = len(token_dict)\n",
        "# token_list = list(token_dict.keys())  # Used for selecting a random word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DqnJnh2ZkqC",
        "outputId": "e8d6c12e-b1b3-4be4-d4f6-053f35dd9d3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "\n",
        "# Build & train the model\n",
        "model = get_model(\n",
        "    token_num=len(token_dict),\n",
        "    head_num=2,\n",
        "    transformer_num=2,\n",
        "    embed_dim=20,\n",
        "    feed_forward_dim=25,\n",
        "    seq_len=maxlen,\n",
        "    pos_num=maxlen,\n",
        "    dropout_rate=0.05,\n",
        ")\n",
        "compile_model(model)\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_9\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Input-Token (InputLayer)        [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Input-Segment (InputLayer)      [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token (TokenEmbedding [(None, 100, 20), (2 5060        Input-Token[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Segment (Embedding)   (None, 100, 20)      40          Input-Segment[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token-Segment (Add)   (None, 100, 20)      0           Embedding-Token[0][0]            \n",
            "                                                                 Embedding-Segment[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Position (PositionEmb (None, 100, 20)      2000        Embedding-Token-Segment[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Dropout (Dropout)     (None, 100, 20)      0           Embedding-Position[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Norm (LayerNormalizat (None, 100, 20)      40          Embedding-Dropout[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, None, 20)     1680        Embedding-Norm[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, None, 20)     0           Encoder-1-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 100, 20)      0           Embedding-Norm[0][0]             \n",
            "                                                                 Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 100, 20)      40          Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward (FeedForw (None, 100, 20)      1045        Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Dropout ( (None, 100, 20)      0           Encoder-1-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Add (Add) (None, 100, 20)      0           Encoder-1-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-1-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Norm (Lay (None, 100, 20)      40          Encoder-1-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, None, 20)     1680        Encoder-1-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, None, 20)     0           Encoder-2-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 100, 20)      0           Encoder-1-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 100, 20)      40          Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward (FeedForw (None, 100, 20)      1045        Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Dropout ( (None, 100, 20)      0           Encoder-2-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Add (Add) (None, 100, 20)      0           Encoder-2-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-2-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Norm (Lay (None, 100, 20)      40          Encoder-2-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "MLM-Dense (Dense)               (None, 100, 20)      420         Encoder-2-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "MLM-Norm (LayerNormalization)   (None, 100, 20)      40          MLM-Dense[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "Extract (Extract)               (None, 20)           0           Encoder-2-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "MLM-Sim (EmbeddingSimilarity)   (None, 100, 253)     253         MLM-Norm[0][0]                   \n",
            "                                                                 Embedding-Token[0][1]            \n",
            "__________________________________________________________________________________________________\n",
            "Input-Masked (InputLayer)       [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "NSP-Dense (Dense)               (None, 20)           420         Extract[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "MLM (Masked)                    (None, 100, 253)     0           MLM-Sim[0][0]                    \n",
            "                                                                 Input-Masked[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "NSP (Dense)                     (None, 2)            42          NSP-Dense[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 13,925\n",
            "Trainable params: 13,925\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tO-0jC3O4at8",
        "outputId": "d8e0864b-3183-4a3f-81f4-8eb37a234172",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "\n",
        "def _generator():\n",
        "    while True:\n",
        "        yield gen_batch_inputs(\n",
        "            training_input,\n",
        "            token_dict,\n",
        "            token_list,\n",
        "            seq_len=maxlen,\n",
        "            mask_rate=0.3,\n",
        "            swap_sentence_rate=0.0, # don't apply sentence swapping\n",
        "        )\n",
        "\n",
        "model.fit_generator(\n",
        "    generator=_generator(),\n",
        "    steps_per_epoch=10,\n",
        "    epochs=3,\n",
        "    validation_data=_generator(),\n",
        "    validation_steps=5,\n",
        "    callbacks=[\n",
        "        keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "    ],\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:432: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 125s 12s/step - loss: 1.9661 - MLM_loss: 1.6284 - NSP_loss: 0.3377 - val_loss: 1.9476 - val_MLM_loss: 1.6308 - val_NSP_loss: 0.3168\n",
            "Epoch 2/3\n",
            "10/10 [==============================] - 124s 12s/step - loss: 1.9688 - MLM_loss: 1.6304 - NSP_loss: 0.3384 - val_loss: 1.9469 - val_MLM_loss: 1.6307 - val_NSP_loss: 0.3161\n",
            "Epoch 3/3\n",
            "10/10 [==============================] - 123s 12s/step - loss: 1.9663 - MLM_loss: 1.6282 - NSP_loss: 0.3380 - val_loss: 1.9458 - val_MLM_loss: 1.6302 - val_NSP_loss: 0.3156\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f6d987f6668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmwWTNrwbMP3"
      },
      "source": [
        "\n",
        "# # Use the trained model\n",
        "# inputs, output_layer = get_model(\n",
        "#     token_num=len(token_dict),\n",
        "#     head_num=5,\n",
        "#     transformer_num=12,\n",
        "#     embed_dim=25,\n",
        "#     feed_forward_dim=100,\n",
        "#     seq_len=maxlen,\n",
        "#     pos_num=maxlen,\n",
        "#     dropout_rate=0.05,\n",
        "#     training=False,      # The input layers and output layer will be returned if `training` is `False`\n",
        "#     trainable=False,     # Whether the model is trainable. The default value is the same with `training`\n",
        "#     output_layer_num=4,  # The number of layers whose outputs will be concatenated as a single output.\n",
        "#                          # Only available when `training` is `False`.\n",
        "# )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrYrfqlgaYOl"
      },
      "source": [
        "## Fine tune the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BEkX42bm-je"
      },
      "source": [
        "# Make a copy of the model\n",
        "classification_model = model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hisSCLK_bKBd",
        "outputId": "5a350f29-7a24-4aa1-84a0-b1c298f5c68b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "classification_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_9\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Input-Token (InputLayer)        [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Input-Segment (InputLayer)      [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token (TokenEmbedding [(None, 100, 20), (2 5060        Input-Token[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Segment (Embedding)   (None, 100, 20)      40          Input-Segment[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token-Segment (Add)   (None, 100, 20)      0           Embedding-Token[0][0]            \n",
            "                                                                 Embedding-Segment[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Position (PositionEmb (None, 100, 20)      2000        Embedding-Token-Segment[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Dropout (Dropout)     (None, 100, 20)      0           Embedding-Position[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Norm (LayerNormalizat (None, 100, 20)      40          Embedding-Dropout[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, None, 20)     1680        Embedding-Norm[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, None, 20)     0           Encoder-1-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 100, 20)      0           Embedding-Norm[0][0]             \n",
            "                                                                 Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 100, 20)      40          Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward (FeedForw (None, 100, 20)      1045        Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Dropout ( (None, 100, 20)      0           Encoder-1-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Add (Add) (None, 100, 20)      0           Encoder-1-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-1-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Norm (Lay (None, 100, 20)      40          Encoder-1-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, None, 20)     1680        Encoder-1-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, None, 20)     0           Encoder-2-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 100, 20)      0           Encoder-1-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 100, 20)      40          Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward (FeedForw (None, 100, 20)      1045        Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Dropout ( (None, 100, 20)      0           Encoder-2-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Add (Add) (None, 100, 20)      0           Encoder-2-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-2-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Norm (Lay (None, 100, 20)      40          Encoder-2-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "MLM-Dense (Dense)               (None, 100, 20)      420         Encoder-2-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "MLM-Norm (LayerNormalization)   (None, 100, 20)      40          MLM-Dense[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "Extract (Extract)               (None, 20)           0           Encoder-2-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "MLM-Sim (EmbeddingSimilarity)   (None, 100, 253)     253         MLM-Norm[0][0]                   \n",
            "                                                                 Embedding-Token[0][1]            \n",
            "__________________________________________________________________________________________________\n",
            "Input-Masked (InputLayer)       [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "NSP-Dense (Dense)               (None, 20)           420         Extract[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "MLM (Masked)                    (None, 100, 253)     0           MLM-Sim[0][0]                    \n",
            "                                                                 Input-Masked[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "NSP (Dense)                     (None, 2)            42          NSP-Dense[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 13,925\n",
            "Trainable params: 13,925\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnk8nHV5bxSX"
      },
      "source": [
        "inputs = classification_model.inputs[:2]\n",
        "dense = classification_model.get_layer('NSP-Dense').output\n",
        "outputs = keras.layers.Dense(units=8, activation='softmax')(dense)\n",
        "\n",
        "classification_model = keras.models.Model(inputs, outputs)\n",
        "classification_model.compile(\n",
        "    \"adam\",\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['sparse_categorical_accuracy', 'accuracy'],\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdhHdwt8cgDa",
        "outputId": "a2733e11-8b34-441f-aa36-548e8bcdde43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "classification_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_13\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Input-Token (InputLayer)        [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Input-Segment (InputLayer)      [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token (TokenEmbedding [(None, 100, 20), (2 5060        Input-Token[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Segment (Embedding)   (None, 100, 20)      40          Input-Segment[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token-Segment (Add)   (None, 100, 20)      0           Embedding-Token[0][0]            \n",
            "                                                                 Embedding-Segment[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Position (PositionEmb (None, 100, 20)      2000        Embedding-Token-Segment[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Dropout (Dropout)     (None, 100, 20)      0           Embedding-Position[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Norm (LayerNormalizat (None, 100, 20)      40          Embedding-Dropout[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, None, 20)     1680        Embedding-Norm[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, None, 20)     0           Encoder-1-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 100, 20)      0           Embedding-Norm[0][0]             \n",
            "                                                                 Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 100, 20)      40          Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward (FeedForw (None, 100, 20)      1045        Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Dropout ( (None, 100, 20)      0           Encoder-1-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Add (Add) (None, 100, 20)      0           Encoder-1-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-1-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Norm (Lay (None, 100, 20)      40          Encoder-1-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, None, 20)     1680        Encoder-1-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, None, 20)     0           Encoder-2-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 100, 20)      0           Encoder-1-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 100, 20)      40          Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward (FeedForw (None, 100, 20)      1045        Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Dropout ( (None, 100, 20)      0           Encoder-2-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Add (Add) (None, 100, 20)      0           Encoder-2-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-2-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Norm (Lay (None, 100, 20)      40          Encoder-2-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Extract (Extract)               (None, 20)           0           Encoder-2-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "NSP-Dense (Dense)               (None, 20)           420         Extract[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 8)            168         NSP-Dense[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 13,338\n",
            "Trainable params: 13,338\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_hfeSxdnGRA"
      },
      "source": [
        "## Prepare input data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCWgLYPergTm"
      },
      "source": [
        "def apply_tokenizer(x):\n",
        "\n",
        "  try:\n",
        "    return TOKEN_DICT[x]\n",
        "  except:\n",
        "    # Assign unknown\n",
        "    print(f\"assigning unknown to {x}\")\n",
        "    return 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQb8RTYQpnkt",
        "outputId": "68cd3f52-4ff6-4a64-c396-cf8bd94859b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "# Apply tokenizer to data\n",
        "# Super hacky!\n",
        "# Need to figure out how to tokenize everything properly\n",
        "\n",
        "TOKEN_DICT = token_dict\n",
        "\n",
        "def split_convert_tokenize(row):\n",
        "\n",
        "  seq =  [int(x) for x in row.split(\" \")]\n",
        "\n",
        "  return [apply_tokenizer(x) for x in seq]\n",
        "\n",
        "train = pd.read_csv(\"/content/drive/My Drive/Research/CyberBERT/data/train_multiclass.csv\")\n",
        "valid = pd.read_csv(\"/content/drive/My Drive/Research/CyberBERT/data/valid_multiclass.csv\")\n",
        "test = pd.read_csv(\"/content/drive/My Drive/Research/CyberBERT/data/test_multiclass.csv\")\n",
        "\n",
        "train[\"calls\"] = train[\"calls\"].apply(lambda x: split_convert_tokenize(x))\n",
        "valid[\"calls\"] = valid[\"calls\"].apply(lambda x: split_convert_tokenize(x))\n",
        "test[\"calls\"] = test[\"calls\"].apply(lambda x: split_convert_tokenize(x))\n",
        "\n",
        "\n",
        "y_train = np.asarray(train[\"label\"])\n",
        "x_train = np.stack(np.asarray(train[\"calls\"]),axis=0)\n",
        "\n",
        "y_val = np.asarray(valid[\"label\"])\n",
        "x_val = np.stack(np.asarray(valid[\"calls\"]),axis=0)\n",
        "\n",
        "y_test = np.asarray(test[\"label\"])\n",
        "x_test = np.stack(np.asarray(test[\"calls\"]),axis=0)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "assigning unknown to 260\n",
            "assigning unknown to 237\n",
            "assigning unknown to 237\n",
            "assigning unknown to 239\n",
            "assigning unknown to 239\n",
            "assigning unknown to 239\n",
            "assigning unknown to 257\n",
            "assigning unknown to 257\n",
            "assigning unknown to 257\n",
            "assigning unknown to 257\n",
            "assigning unknown to 257\n",
            "assigning unknown to 239\n",
            "assigning unknown to 239\n",
            "assigning unknown to 239\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdSVu1K0mrtg"
      },
      "source": [
        "def format_data(x_train):\n",
        "    return [x_train, np.zeros_like(x_train)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xT_s72aFn9hE"
      },
      "source": [
        "x_train_formatted = format_data(x_train)\n",
        "x_val_formatted = format_data(x_val)\n",
        "x_test_formatted = format_data(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNz9KT-Oe4n0",
        "outputId": "28376531-5824-4399-ec0a-c8654d88769e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "history = classification_model.fit(\n",
        "    x_train_formatted, y_train, epochs=10, batch_size=maxlen, validation_data=(x_val_formatted,y_val)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "40/40 [==============================] - 11s 280ms/step - loss: 2.0744 - sparse_categorical_accuracy: 0.1807 - accuracy: 0.1807 - val_loss: 1.9672 - val_sparse_categorical_accuracy: 0.2588 - val_accuracy: 0.2588\n",
            "Epoch 2/10\n",
            "40/40 [==============================] - 10s 250ms/step - loss: 1.9124 - sparse_categorical_accuracy: 0.2892 - accuracy: 0.2892 - val_loss: 1.8691 - val_sparse_categorical_accuracy: 0.3069 - val_accuracy: 0.3069\n",
            "Epoch 3/10\n",
            "40/40 [==============================] - 10s 261ms/step - loss: 1.8050 - sparse_categorical_accuracy: 0.3305 - accuracy: 0.3305 - val_loss: 1.8263 - val_sparse_categorical_accuracy: 0.3358 - val_accuracy: 0.3358\n",
            "Epoch 4/10\n",
            "40/40 [==============================] - 10s 258ms/step - loss: 1.7053 - sparse_categorical_accuracy: 0.3825 - accuracy: 0.3825 - val_loss: 1.7677 - val_sparse_categorical_accuracy: 0.3681 - val_accuracy: 0.3681\n",
            "Epoch 5/10\n",
            "40/40 [==============================] - 10s 254ms/step - loss: 1.6259 - sparse_categorical_accuracy: 0.4245 - accuracy: 0.4245 - val_loss: 1.7531 - val_sparse_categorical_accuracy: 0.3833 - val_accuracy: 0.3833\n",
            "Epoch 6/10\n",
            "40/40 [==============================] - 10s 256ms/step - loss: 1.5352 - sparse_categorical_accuracy: 0.4633 - accuracy: 0.4633 - val_loss: 1.7076 - val_sparse_categorical_accuracy: 0.3917 - val_accuracy: 0.3917\n",
            "Epoch 7/10\n",
            "40/40 [==============================] - 10s 259ms/step - loss: 1.4671 - sparse_categorical_accuracy: 0.4882 - accuracy: 0.4882 - val_loss: 1.7069 - val_sparse_categorical_accuracy: 0.4005 - val_accuracy: 0.4005\n",
            "Epoch 8/10\n",
            "40/40 [==============================] - 10s 257ms/step - loss: 1.4100 - sparse_categorical_accuracy: 0.5085 - accuracy: 0.5085 - val_loss: 1.6537 - val_sparse_categorical_accuracy: 0.4279 - val_accuracy: 0.4279\n",
            "Epoch 9/10\n",
            "40/40 [==============================] - 10s 257ms/step - loss: 1.3374 - sparse_categorical_accuracy: 0.5337 - accuracy: 0.5337 - val_loss: 1.6295 - val_sparse_categorical_accuracy: 0.4471 - val_accuracy: 0.4471\n",
            "Epoch 10/10\n",
            "14/40 [=========>....................] - ETA: 5s - loss: 1.2739 - sparse_categorical_accuracy: 0.5679 - accuracy: 0.5679"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rN8Xwb7isP3d"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.layers import LSTM, Dense, Dropout, Embedding\n",
        "from keras.preprocessing import sequence\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers import SpatialDropout1D\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "import matplotlib.pyplot as plt "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3azfL_foe4k7"
      },
      "source": [
        "y_test_pred = classification_model.predict(x_test_formatted)\n",
        "y_classes = y_test_pred.argmax(axis=-1)\n",
        "cm = confusion_matrix(y_test, y_classes)\n",
        "\n",
        "plot_confusion_matrix(conf_mat=cm,\n",
        "                      show_absolute=True,\n",
        "                      show_normed=True,\n",
        "                      colorbar=True)\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.grid()\n",
        "plt.savefig(\"accuracy.png\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.grid()\n",
        "plt.savefig(\"loss.png\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WouCZyE2dm1J"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}