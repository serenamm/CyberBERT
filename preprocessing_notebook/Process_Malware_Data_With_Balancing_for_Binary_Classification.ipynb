{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Process_Malware_Data_With_Balancing_for_Binary_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOmAXOX_tFVL"
      },
      "source": [
        "# About\n",
        "\n",
        "The paper that we are comparing to uses a dataset with 8 malware classes. They use this malware dataset in two ways:\n",
        "\n",
        "## 1) Multiple binary classifications:\n",
        "\n",
        "Let's use adware as an example. Adware is a class label in the dataset. The researchers frame this as a binary classification case where they set all non-adware instances to have a label of 0, with adware instances having a label of 1. They then train a classifier to predict adware or not.\n",
        "\n",
        "They do this for all classes. This results in 8 separate classifiers, each trained to determine one class.\n",
        "\n",
        "## 2) Multi-class classification\n",
        "\n",
        "The authors also develop a multi-class classification model, which is tasked to predict the class of adware, out of 8 possible classes.\n",
        "\n",
        "\n",
        "# This notebook\n",
        "\n",
        "In this notebook, we pre-process the data into the proper form for binary classification (outputs 8 dataframes, one for each problem, with 2 labels [0 or 1] for each), as well as the proper form for multi-class classification (one dataframe with 8 possible labels).\n",
        "\n",
        "The researchers originally do not use a validation set. We will use a validation set as that is proper practice. However, we will keep the test set the same as theirs to allow for a fair comparison (even though we are running the same models that they did).\n",
        "\n",
        "For the binary classification case, the dataset is unbalanced. We randomly oversample the training set mitigate this but leave the test set alone.\n",
        "\n",
        "To create a validation set we simply split the train set in two. This could be improved upon but perhaps is not needed to be."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBGJia-Gh13Z",
        "outputId": "a9dff30b-2039-41d5-f847-e843a44b9c43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "er4O-aYwTkE1"
      },
      "source": [
        "# Set data path, and set parameters for preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YzomT7sTPPh"
      },
      "source": [
        "raw_data_path = '/content/drive/My Drive/Research/CyberBERT/data'\n",
        "destination_folder = '/content/drive/My Drive/Research/CyberBERT/model'\n",
        "\n",
        "# train_test_ratio = 0.10\n",
        "# train_valid_ratio = 0.80\n",
        "\n",
        "# first_n_words = 200"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSbCqzcDWn-4"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-muOECSWrOt"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "import importlib\n",
        "\n",
        "from itertools import chain\n",
        "from keras import backend as K\n",
        "from keras.models import load_model, Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.layers import LSTM, Dense, Dropout, Embedding\n",
        "from keras.preprocessing import sequence\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNfpHFEPUcyE"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDW59vc2icw1"
      },
      "source": [
        "# Read in data\n",
        "## Calls: the API calls made by the malware\n",
        "malware_calls_df = pd.read_csv(f\"{raw_data_path}/calls.zip\", compression=\"zip\",\n",
        "                               sep=\"\\t\", names=[\"API_Calls\"])\n",
        "## Labels (types of malware)\n",
        "malware_labels_df = pd.read_csv(f\"{raw_data_path}/types.zip\", compression=\"zip\",\n",
        "                               sep=\"\\t\", names=[\"API_Labels\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7jafLVojHG1"
      },
      "source": [
        "malware_calls_df[\"API_Labels\"] = malware_labels_df.API_Labels\n",
        "malware_calls_df[\"API_Calls\"] = malware_calls_df.API_Calls.apply(lambda x: \" \".join(x.split(\",\")))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-b_az2GhpZz",
        "outputId": "4ceb8865-9c51-41eb-cfc3-32543cc7d0bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "malware_calls_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>API_Calls</th>\n",
              "      <th>API_Labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>292 291 292 291 291 291 291 291 291 291 291 29...</td>\n",
              "      <td>Trojan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>278 192 199 192 290 291 291 291 291 290 291 29...</td>\n",
              "      <td>Trojan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>290 291 51 34 232 238 220 221 220 69 69 66 80 ...</td>\n",
              "      <td>Backdoor</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>292 291 292 291 291 291 291 291 291 291 291 29...</td>\n",
              "      <td>Backdoor</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>292 291 291 291 291 291 291 291 291 291 291 29...</td>\n",
              "      <td>Trojan</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           API_Calls API_Labels\n",
              "0  292 291 292 291 291 291 291 291 291 291 291 29...     Trojan\n",
              "1  278 192 199 192 290 291 291 291 291 290 291 29...     Trojan\n",
              "2  290 291 51 34 232 238 220 221 220 69 69 66 80 ...   Backdoor\n",
              "3  292 291 292 291 291 291 291 291 291 291 291 29...   Backdoor\n",
              "4  292 291 291 291 291 291 291 291 291 291 291 29...     Trojan"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqdiFnW7i-XK"
      },
      "source": [
        "labels = malware_calls_df.API_Labels.unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POKv2gGkjwsZ"
      },
      "source": [
        "def preprocess_binary_data(input_df, class_label):\n",
        "  \"\"\"Preprocess data for binary classification and output a train, test dataframe.\n",
        "  Given a class label, and an input dataframe, label every instance with 0\n",
        "  if the instance is not from the class_label target, and 1 otherwise.\n",
        "  \n",
        "  Parameters\n",
        "  ----------\n",
        "  input_df: pd.DataFrame\n",
        "    the input dataframe, contains all data (not split into train/valid/test)\n",
        "  class_label: str\n",
        "    the class label for which we're creating the train and test set.\n",
        "    e.g., if class label is \"Adware\" then we're creating a dataset where\n",
        "    the only instances with a 1 label are those that correspond to adware.\n",
        "  \"\"\"\n",
        "\n",
        "  df = input_df.copy()\n",
        "  print(f\"Labelling {class_label} df\")\n",
        "  df[\"API_Labels\"] = df.API_Labels.apply(lambda x: 1 if x == class_label else 0)\n",
        "  max_words = 800\n",
        "  max_len = 100\n",
        "\n",
        "  X = df.API_Calls\n",
        "  Y = df.API_Labels.astype('category').cat.codes\n",
        "\n",
        "  tok = Tokenizer(num_words=max_words)\n",
        "  tok.fit_on_texts(X)\n",
        "  print('Found %s unique tokens.' % len(tok.word_index))\n",
        "  X = tok.texts_to_sequences(X.values)\n",
        "  X = sequence.pad_sequences(X, maxlen=max_len)\n",
        "  print('Shape of data tensor:', X.shape)\n",
        "\n",
        "  # Note: this test_size is set as 0.15 since the original paper\n",
        "  # uses a split like this for the test set\n",
        "  # the original paper's code also does not have a validation set,\n",
        "  # so we'll manually create that ourselves\n",
        "  X_train, X_test, Y_train, Y_test = train_test_split(X, Y,\n",
        "                                                    test_size=0.15)\n",
        "  \n",
        "  data_dict = {\n",
        "    \"calls\": list(X_train),\n",
        "    \"label\": list(Y_train)\n",
        "  }\n",
        "\n",
        "  train_df = pd.DataFrame(data_dict, columns=['label', \"calls\"])\n",
        "  train_df[\"calls\"] = train_df['calls'].apply(lambda x: ' '.join(map(str, x)))\n",
        "  data_dict = {\n",
        "      \"calls\": list(X_test),\n",
        "      \"label\": list(Y_test)\n",
        "  }\n",
        "\n",
        "  test_df = pd.DataFrame(data_dict, columns=['label', \"calls\"])\n",
        "  test_df[\"calls\"] = test_df['calls'].apply(lambda x: ' '.join(map(str, x)))  \n",
        "  \n",
        "  print(f\"Writing {class_label} df to disk\")\n",
        "\n",
        "  # Write test dataframe\n",
        "  test_df.to_csv(f\"/content/drive/My Drive/Research/CyberBERT/data/test_{class_label}.csv\", index=False)\n",
        "\n",
        "  return train_df, test_df\n",
        "\n",
        "def balance_binary_data(train_df, class_label):\n",
        "  \"\"\"Balance an input dataframe.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  train_df: pd.DataFrame\n",
        "    the training dataframe that needs to be balanced.\n",
        "  class_label: str\n",
        "    the class label for which we're balancing the data\n",
        "  Note\n",
        "  ----\n",
        "  The dataset is very unbalanced when we consider the binary\n",
        "  classification case only. For the multi-class classification case,\n",
        "  we will not rebalance.\n",
        "  \"\"\"\n",
        "\n",
        "  print(f\"balancing {class_label} df\")\n",
        "  count_class_0, count_class_1 = train_df.label.value_counts()\n",
        "\n",
        "  # Divide by class\n",
        "  df_class_0 = train_df[train_df['label'] == 0]\n",
        "  df_class_1 = train_df[train_df['label'] == 1]\n",
        "\n",
        "  df_class_1_over = df_class_1.sample(count_class_0, replace=True)\n",
        "  df_train_over = pd.concat([df_class_0, df_class_1_over], axis=0)\n",
        "\n",
        "  print('Random over-sampling:')\n",
        "  print(df_train_over.label.value_counts())\n",
        "\n",
        "  print(f\"writing over sampled {class_label} df\")\n",
        "\n",
        "  df_train_over = df_train_over.sample(frac=1, random_state=42)\n",
        "  # create a validation set as well\n",
        "  df_train_over[:4000].to_csv(f\"/content/drive/My Drive/Research/CyberBERT/data/train_over_{class_label}.csv\", index=False)\n",
        "  df_train_over[4000:].to_csv(f\"/content/drive/My Drive/Research/CyberBERT/data/valid_over_{class_label}.csv\", index=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b02dYEMejDsy"
      },
      "source": [
        "# We will process each dataset separately - not the most efficient, but it fits better with the current work flow.\n",
        "def preprocess_and_balance_binary_data(input_df, class_label):\n",
        "\n",
        "  train_df, test_df = preprocess_binary_data(input_df, class_label)\n",
        "\n",
        "  balance_binary_data(train_df, class_label)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eS_WBVFkbyP",
        "outputId": "015e33ec-fe69-49d9-9ec1-f613b20edf47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for class_label in labels:\n",
        "  preprocess_and_balance_binary_data(malware_calls_df, class_label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Labelling Trojan df\n",
            "Found 278 unique tokens.\n",
            "Shape of data tensor: (7107, 100)\n",
            "Writing Trojan df to disk\n",
            "balancing Trojan df\n",
            "Random over-sampling:\n",
            "1    5195\n",
            "0    5195\n",
            "Name: label, dtype: int64\n",
            "writing over sampled Trojan df\n",
            "Labelling Backdoor df\n",
            "Found 278 unique tokens.\n",
            "Shape of data tensor: (7107, 100)\n",
            "Writing Backdoor df to disk\n",
            "balancing Backdoor df\n",
            "Random over-sampling:\n",
            "1    5184\n",
            "0    5184\n",
            "Name: label, dtype: int64\n",
            "writing over sampled Backdoor df\n",
            "Labelling Downloader df\n",
            "Found 278 unique tokens.\n",
            "Shape of data tensor: (7107, 100)\n",
            "Writing Downloader df to disk\n",
            "balancing Downloader df\n",
            "Random over-sampling:\n",
            "1    5201\n",
            "0    5201\n",
            "Name: label, dtype: int64\n",
            "writing over sampled Downloader df\n",
            "Labelling Worms df\n",
            "Found 278 unique tokens.\n",
            "Shape of data tensor: (7107, 100)\n",
            "Writing Worms df to disk\n",
            "balancing Worms df\n",
            "Random over-sampling:\n",
            "1    5192\n",
            "0    5192\n",
            "Name: label, dtype: int64\n",
            "writing over sampled Worms df\n",
            "Labelling Spyware df\n",
            "Found 278 unique tokens.\n",
            "Shape of data tensor: (7107, 100)\n",
            "Writing Spyware df to disk\n",
            "balancing Spyware df\n",
            "Random over-sampling:\n",
            "1    5329\n",
            "0    5329\n",
            "Name: label, dtype: int64\n",
            "writing over sampled Spyware df\n",
            "Labelling Adware df\n",
            "Found 278 unique tokens.\n",
            "Shape of data tensor: (7107, 100)\n",
            "Writing Adware df to disk\n",
            "balancing Adware df\n",
            "Random over-sampling:\n",
            "1    5720\n",
            "0    5720\n",
            "Name: label, dtype: int64\n",
            "writing over sampled Adware df\n",
            "Labelling Dropper df\n",
            "Found 278 unique tokens.\n",
            "Shape of data tensor: (7107, 100)\n",
            "Writing Dropper df to disk\n",
            "balancing Dropper df\n",
            "Random over-sampling:\n",
            "1    5284\n",
            "0    5284\n",
            "Name: label, dtype: int64\n",
            "writing over sampled Dropper df\n",
            "Labelling Virus df\n",
            "Found 278 unique tokens.\n",
            "Shape of data tensor: (7107, 100)\n",
            "Writing Virus df to disk\n",
            "balancing Virus df\n",
            "Random over-sampling:\n",
            "1    5180\n",
            "0    5180\n",
            "Name: label, dtype: int64\n",
            "writing over sampled Virus df\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uylsxY5Nl0cw"
      },
      "source": [
        "def preprocess_multiclass_data(input_df):\n",
        "\n",
        "  # Don't over sample\n",
        "  df = input_df.copy()\n",
        "  print(\"Labelling multiclass df\")\n",
        "\n",
        "  max_words = 800\n",
        "  max_len = 100\n",
        "\n",
        "  X = df.API_Calls\n",
        "\n",
        "  Y = df.API_Labels.astype('category').cat.codes\n",
        "  category_list = df.API_Labels.astype('category').cat.categories\n",
        "  category_dict = dict()\n",
        "  for x in range(len(category_list)):\n",
        "    category_dict[x] = category_list[x]\n",
        "\n",
        "  import json\n",
        "  # Write category dict so we can map labels back to categories later\n",
        "  with open('category_dict.json', 'w') as f:\n",
        "      json.dump(category_dict, f)\n",
        "\n",
        "  tok = Tokenizer(num_words=max_words)\n",
        "  tok.fit_on_texts(X)\n",
        "  print('Found %s unique tokens.' % len(tok.word_index))\n",
        "  X = tok.texts_to_sequences(X.values)\n",
        "  X = sequence.pad_sequences(X, maxlen=max_len)\n",
        "  print('Shape of data tensor:', X.shape)\n",
        "\n",
        "  X_train, X_test, Y_train, Y_test = train_test_split(X, Y,\n",
        "                                                    test_size=0.15)\n",
        "  \n",
        "  data_dict = {\n",
        "    \"calls\": list(X_train),\n",
        "    \"label\": list(Y_train)\n",
        "  }\n",
        "\n",
        "  train_df = pd.DataFrame(data_dict, columns=['label', \"calls\"])\n",
        "  train_df[\"calls\"] = train_df['calls'].apply(lambda x: ' '.join(map(str, x)))\n",
        "  data_dict = {\n",
        "      \"calls\": list(X_test),\n",
        "      \"label\": list(Y_test)\n",
        "  }\n",
        "\n",
        "  test_df = pd.DataFrame(data_dict, columns=['label', \"calls\"])\n",
        "  test_df[\"calls\"] = test_df['calls'].apply(lambda x: ' '.join(map(str, x)))  \n",
        "  \n",
        "  print(\"Writing multiclass df to disk\")\n",
        "\n",
        "  # Write test dataframe\n",
        "  # create a validation set\n",
        "  train_df[:4000].to_csv(\"/content/drive/My Drive/Research/CyberBERT/data/train_multiclass.csv\", index=False)\n",
        "\n",
        "  train_df[4000:].to_csv(\"/content/drive/My Drive/Research/CyberBERT/data/valid_multiclass.csv\", index=False)\n",
        "\n",
        "  test_df.to_csv(\"/content/drive/My Drive/Research/CyberBERT/data/test_multiclass.csv\", index=False)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQMeOEU2m8va",
        "outputId": "6581a339-304e-4224-b263-4d66a63d97ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "preprocess_multiclass_data(malware_calls_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Labelling multiclass df\n",
            "Found 278 unique tokens.\n",
            "Shape of data tensor: (7107, 100)\n",
            "Writing multiclass df to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0njjCKWbgAY"
      },
      "source": [
        "# Create vocab list\n",
        "\n",
        "A vocabularly list is needed to train the BERT model.\n",
        "\n",
        "This stays the same regardless of dataset label. So we can do it once."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rlhWq8ZvXwe"
      },
      "source": [
        "X = malware_calls_df.API_Calls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNC5YB5-Z5-7"
      },
      "source": [
        "vocab_list = []\n",
        "for i in range(len(X)):\n",
        "  call_list = X[i]\n",
        "  call_list = call_list.split(\" \")\n",
        "  call_list = list(set(call_list))\n",
        "  to_add = [x for x in call_list if x not in vocab_list]\n",
        "  vocab_list.append(to_add)\n",
        "# Add special tokens to vocab\n",
        "vocab_list.append(['SEP', 'PAD', 'UNK', 'MASK', 'CLS'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBvNydDya_5S"
      },
      "source": [
        "from itertools import chain\n",
        "\n",
        "vocab_list = list(set(list(chain(*vocab_list))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UObEBuRYcRhp",
        "outputId": "db4fca1a-5d61-45fd-f068-aa26bac3edba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# There are 283 tokens, including the special tokens\n",
        "len(vocab_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "283"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVH-0kTKmgtc"
      },
      "source": [
        "with open('/content/drive/My Drive/Research/CyberBERT/data/vocab.txt', 'w') as f:\n",
        "    for item in vocab_list:\n",
        "        f.write(f\"[{item}]\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNSGW2j8ntZr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}